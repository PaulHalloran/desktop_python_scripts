import numpy as np
import numpy.ma as ma
import matplotlib.pyplot as plt
import iris
import glob
import iris.experimental.concatenate
import iris.analysis
import iris.quickplot as qplt
import iris.analysis.cartography
import cartopy.crs as ccrs
import subprocess
from iris.coords import DimCoord
import iris.coord_categorisation
import matplotlib as mpl

def regrid_data(files):
    variable_name = files[0].split('_')[1].split('/')[2]
    for file in files:
        start = '/'.join(file.split('/')[1:6])
        end = file.split('/')[-1]
        subprocess.Popen("cdo remapbil,r360x180 -selname,"+variable_name+" "+file+" /"+start+"/regridded/"+end,shell=True)
        #p.wait()
        #putting on common 1x1 degree grids

def regrid_data2(file):
    variable_name = file.split('_')[1].split('/')[2]
    start = '/'.join(file.split('/')[1:6])
    end = file.split('/')[-1]
    p = subprocess.Popen("cdo remapbil,r360x180 -selname,"+variable_name+" "+file+" /"+start+"/regridded/"+end,shell=True)
    p.wait()
    return True
        #putting on common 1x1 degree grids

def my_callback(cube, field,files):
    # there are some bad attributes in the NetCDF files which make the data incompatible for merging.
    cube.attributes.pop('history')
    cube.attributes.pop('tracking_id')
    cube.attributes.pop('creation_date')
    cube.attributes.pop('processed_by')
    #if np.size(cube) > 1:
    #cube = iris.experimental.concatenate.concatenate(cube)
    return cube


directory = '/data/data0/ph290/cmip5_data/'

alk_files = np.array(glob.glob(directory+'talk/*.nc'))
so_files = np.array(glob.glob(directory+'so/*.nc'))


'''
regrid files - note, only need to do once
'''


# no_files = np.size(alk_files)
# steps = 8.0

# for i in np.arange(no_files/steps):
#     print 'count = '+np.str(i)
#     files=np.array(alk_files[i*steps:np.min([(i+1)*steps-1,no_files-1])])
#     regrid_data(files)
#     files=alk_files[np.min([(i+1)*steps,no_files-1])]
#     answer = regrid_data2(files)
#     #note, having a second thing here is just a poor way of getting it to wait until some of the subprocesses have finished (in fact it is a poor way of doing this, because this process may be longer/shorter than the others, so it could fly through again, and get loads of them going. shoudl hopefully stop it from trying to regrid all o fthe files in parallel though...)

# no_files = np.size(so_files)
# steps = 8.0

# for i in np.arange(no_files/steps):
#     print 'count = '+np.str(i)
#     files=np.array(alk_files[i*steps:np.min([(i+1)*steps-1,no_files-1])])
#     regrid_data(files)
#     files=alk_files[np.min([(i+1)*steps,no_files-1])]
#     answer = regrid_data2(files)

'''
Read in 1x1 degree basin masks from WOA
'''

woa_dir = '/home/ph290/Documents/teaching/'
basin_mask = iris.load_cube(woa_dir+'basin.nc')
basin_mask_tmp = basin_mask[0][0]
basin_mask_tmp.data[np.where(np.logical_not(basin_mask_tmp.data == 1))] = np.nan
# 1 = Atlantic
# 2 = Pacific
# 3 = Indian
# 4 = Southen Ocean
# 5 = Arctic
# if upside down:
# basin_mask_flipped = iris.analysis.maths.np.flipud(basin_mask.data)
# apply mask with:
# sstb = iris.analysis.maths.multiply(sst,basin_mask_flipped)

'''
Read in GLODAP and WOA data
'''

qlodap_dir = '/home/ph290/data1/observations/glodap/'
woa_dir = '/home/ph290/Documents/teaching/'
alk_in = iris.load_cube(qlodap_dir+'PALK.nc','Potential_Alkalinity')
alk_in = iris.analysis.maths.divide(alk_in,1000.0) # convert units
alk_in.transpose((0,2,1)) #reorders dimenions to be same as CMIP5 and WOA
salinity_in = iris.load_cube(woa_dir+'salinity_annual_1deg.nc','sea_water_salinity')




'''
Reading file details (model name etc)
'''

alk_files_regrid = np.array(glob.glob(directory+'talk/regridded/*.nc'))

model_names = []
for file in alk_files_regrid:
    tmp = file.split('/')[-1].split('_')
    model_names.append(tmp[2])
  

model_names = np.array(model_names)
model_names_unique = np.unique(model_names)

alk_cubes = []
#ultimately a set of 3D cubes with control run mean alkalinity 

for model in model_names_unique:
    print model
    files = np.array(glob.glob(directory+'talk/regridded/*'+model+'*.nc'))
    cubes = iris.load(files)
    #,'sea_water_alkalinity_expressed_as_mole_equivalent',callback=my_callback)
    cube = iris.experimental.concatenate.concatenate(cubes)
    alk_cubes.append(cube[0].collapsed('time',iris.analysis.MEAN))

basins = np.array([0,1,2,3,10,11])
basin_names = np.array(['Global','Atlantic','Pacific','Indian','Southern Ocean','Arctic Ocean'])

'''
Calculate CMIP5 profiles
'''

regional_profiles = []
regions = ['']

for i in basins:
    model_profiles = []
    for cube in alk_cubes:
        if not cube.coord('latitude').has_bounds():
            cube.coord('latitude').guess_bounds()
        if not cube.coord('longitude').has_bounds():
            cube.coord('longitude').guess_bounds()
        grid_areas = iris.analysis.cartography.area_weights(cube)
        basin_mask_tmp = basin_mask[0][0]
        if i == 0:
            #global case
            loc1 = np.where(np.logical_not(basin_mask_tmp.data >= 1))
            loc2 = np.where(basin_mask_tmp.data >= 1)
        else:
            #all other cases
            loc1 = np.where(np.logical_not(basin_mask_tmp.data == i))
            loc2 = np.where(basin_mask_tmp.data == i)
        basin_mask_tmp.data[loc1] = 0.0
        basin_mask_tmp.data[loc2] = 1.0
        basin_mask_flipped = basin_mask_tmp
        basin_mask_flipped.data = iris.analysis.maths.np.flipud(basin_mask_tmp.data)
        cube_tmp = cube
        cube_tmp = iris.analysis.maths.multiply(cube,basin_mask_flipped)
        cube_tmp.data = ma.masked_equal(cube_tmp.data,0)
        data_shape = np.shape(cube_tmp.data)
        a = ma.getmaskarray(alk_in[0].data)
        a =np.roll(a,180,axis=1)
        a2 = np.tile(a,(data_shape[0],1,1))
        b = ma.getmaskarray(cube_tmp.data)
        mask = a2 | b
        #by combining the masks above, the CMIP5 data has exactky the same land sea mask as the WOA data
        cube_tmp.data = ma.masked_array(cube_tmp.data,mask)
        #qplt.contourf(cube_tmp[0])
        #plt.gca().coastlines()
        #plt.show()
        model_profiles.append(cube_tmp.collapsed(['latitude','longitude'], iris.analysis.MEAN,weights = grid_areas))
    regional_profiles.append(model_profiles)

'''
Calculate GLODAP/WOA profiles
'''

#alk_in = iris.load_cube(qlodap_dir+'PALK.nc','Potential_Alkalinity')
#salinity_in = iris.load_cube(woa_dir+'salinity_annual_1deg.nc','sea_water_salinity')

cube = alk_in

obs_profiles = []
for i in basins:
    if not cube.coord('latitude').has_bounds():
        cube.coord('latitude').guess_bounds()
    if not cube.coord('longitude').has_bounds():
        cube.coord('longitude').guess_bounds()
    grid_areas = iris.analysis.cartography.area_weights(cube)
    basin_mask_tmp = basin_mask[0][0]
    if i == 0:
        #global case
        loc1 = np.where(np.logical_not(basin_mask_tmp.data >= 1))
        loc2 = np.where(basin_mask_tmp.data >= 1)
    else:
        #all other cases
        loc1 = np.where(np.logical_not(basin_mask_tmp.data == i))
        loc2 = np.where(basin_mask_tmp.data == i)
    basin_mask_tmp.data[loc1] = 0.0
    basin_mask_tmp.data[loc2] = 1.0
    basin_mask_flipped = basin_mask_tmp
    basin_mask_flipped.data = iris.analysis.maths.np.flipud(basin_mask_tmp.data)
    basin_mask_flipped.data = iris.analysis.maths.np.roll(basin_mask_tmp.data,180,axis=1)
    cube_tmp = cube
    cube_tmp = iris.analysis.maths.multiply(cube,basin_mask_flipped)
    cube_tmp.data = ma.masked_equal(cube_tmp.data,0)
    #qplt.contourf(cube_tmp[0])
    #plt.gca().coastlines()
    #plt.show()
    obs_profiles.append(cube_tmp.collapsed(['latitude','longitude'], iris.analysis.MEAN,weights = grid_areas))


'''
Plot
'''

linestyles = ['-', '-.', '--', ':','-', '-.', '--', ':','-', '-.', '--', ':','-', '-.', '--', ':','-', '-.', '--', ':','-', '-.', '--', ':']

for i,profiles in enumerate(regional_profiles):
    print i
    for j,model_profile in enumerate(profiles):
        print model_profile.coord
        try:
            depths = model_profile.coord('ocean depth coordinate').points
        except iris.exceptions.CoordinateNotFoundError:
            print 'depth name not ocean depth coordinate'
        try:
            depths = model_profile.coord('depth').points
        except iris.exceptions.CoordinateNotFoundError:
            print 'depth name not depth'
        try:
            depths = model_profile.coord('ocean sigma over z coordinate').points
        except iris.exceptions.CoordinateNotFoundError:
            print 'depth name not ocean sigma over z coordinate'
        print model_profile.data
        line = plt.plot(model_profile.data,depths*(-1.0))
        plt.setp(line, linestyle=linestyles[j],linewidth = 2)
        #plt.title(basin_names[i])
    #qplt.plot(obs_profiles[i],'k',linewidth = 3)
    #print 'obs next'
    #print obs_profiles[i].data
    plt.plot(obs_profiles[i].data,obs_profiles[i].coord('depth').points*(-1.0),'k',linewidth = 3)
    plt.title(basin_names[i])
    plt.show()

for j,model_profile in enumerate(profiles):
    line = plt.plot([0,1],[j+1,j+1])
    plt.text(1.2, j+0.8, model_names_unique[j],fontsize=12)
    plt.setp(line, linestyle=linestyles[j],linewidth = 2)
    plt.xlim([0,2])
    plt.ylim([0,18])

plt.show()

#now need to work out a basin mask, then calculate profiles, and do the salinity side of things...

